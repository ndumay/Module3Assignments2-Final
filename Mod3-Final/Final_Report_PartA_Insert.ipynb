{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174dd716",
   "metadata": {},
   "source": [
    "### Reflection on Milestone 1: Redundant Feature Groups\n",
    "\n",
    "During Milestone 1, we examined several highly correlated feature groups — particularly the square footage (`finishedsquarefeet*`) and bathroom count variants. While we visualized their correlation, we retained most of them in the final dataset.\n",
    "\n",
    "In retrospect, consolidating these into a single representative feature or engineering a composite (e.g., averaging or applying PCA) could have reduced dimensionality, improved model generalization, and decreased noise. Gradient Boosting handled the redundancy well, but this still represents an opportunity for simplification and interpretability in future iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb5b66",
   "metadata": {},
   "source": [
    "### Reflection on Milestone 2: Evaluation Strategy\n",
    "\n",
    "In Milestone 2, we evaluated model performance using a single train-test split and reported RMSE. While this approach was practical and consistent, using a cross-validation strategy like K-Fold would have produced more stable and representative performance estimates.\n",
    "\n",
    "Cross-validation mitigates the risk of overfitting to a particular split and would have helped us better understand variability across samples. This is especially valuable with a moderately sized dataset like ours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f10af",
   "metadata": {},
   "source": [
    "### Final Model: Gradient Boosting Pipeline\n",
    "\n",
    "The following code reproduces the full modeling pipeline using our best-performing model — Gradient Boosting. It integrates key preprocessing steps and reflects on design improvements identified during this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59888f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Final Model Pipeline: Gradient Boosting =====================\n",
    "# This code fulfills Final Report Part A requirement 3.\n",
    "# It runs our selected model using the cleaned data and reflects our Milestone 1 & 2 learnings.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assume df_encoded is our preprocessed feature set from Milestone 1\n",
    "X = df_encoded.drop(columns='taxvaluedollarcnt')\n",
    "y = df_encoded['taxvaluedollarcnt']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "final_model = GradientBoostingRegressor(n_estimators=25, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Final Gradient Boosting RMSE: {rmse:.2f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
