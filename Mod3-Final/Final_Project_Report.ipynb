{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706edba9-eb08-4f57-bcc5-a1bb7024f068",
   "metadata": {},
   "source": [
    "# Project Final Report\n",
    "\n",
    "### Due: Midnight on April 27 (2-hour grace period) — 50 points  \n",
    "\n",
    "### No late submissions will be accepted.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Your final submission consists of **three components**:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Final Report Notebook [40 pts]\n",
    "\n",
    "Complete all sections of this notebook to document your final decisions, results, and broader context.\n",
    "\n",
    "- **Part A**: Select the single best model from your Milestone 2 experiments. Now that you’ve finalized your model, revisit your decisions from Milestones 1 and 2. Are there any steps you would change—such as cleaning, feature engineering, or model evaluation—given what you now know?\n",
    "\n",
    "- **Part B**: Write a technical report following standard conventions, for example:\n",
    "  - [CMU guide to structure](https://www.stat.cmu.edu/~brian/701/notes/paper-structure.pdf)\n",
    "  - [Data science report example](https://www.projectpro.io/article/data-science-project-report/620)\n",
    "  - The Checklist given in this week's Blackboard Lesson (essentially the same as in HOML).\n",
    "    \n",
    "  Your audience here is technically literate but unfamiliar with your work—like your manager or other data scientists. Be clear, precise, and include both code (for illustration), charts/plots/illustrations, and explanation of what you discovered and your reasoning process. \n",
    "\n",
    "The idea here is that Part A would be a repository of the most important code, for further work to come, and Part B is\n",
    "the technical report which summarizes your project for the data science group at your company. Do NOT assume that readers of Part B are intimately familiar with Part A; provide code for illustration as needed, but not to run.\n",
    "\n",
    "Submit this notebook as a group via your team leader’s Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. PowerPoint Presentation [10 pts]\n",
    "\n",
    "Create a 10–15 minute presentation designed for a general audience (e.g., sales or marketing team).\n",
    "\n",
    "- Prepare 8–12 slides, following the general outline of the sections of Part B. \n",
    "- Focus on storytelling, visuals (plots and illustrations), and clear, simplified language. No code!\n",
    "- Use any presentation tool you like, but upload a PDF version.\n",
    "- List all team members on the first slide.\n",
    "\n",
    "Submit as a group via your team leader’s Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Individual Assessment\n",
    "\n",
    "Each team member must complete the Individual Assessment Form (same as in Milestone 1), sign it, and upload it via their own Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "-  Final Report Notebook — Team leader submission\n",
    "-  PDF Slides — Team leader submission\n",
    "-  Individual Assessment Form — Each member submits their own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee15ea-7cf4-4d57-b83a-18f186f0d204",
   "metadata": {},
   "source": [
    "## Part A: Final Model and Design Reassessment [10 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model and revisit earlier decisions to determine if any should be revised in light of your complete modeling workflow. You’ll also consolidate and present the key code used to run your model on the preprocessed dataset, with thoughtful documentation of your reasoning.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Reconsider **at least one decision from Milestone 1** (e.g., preprocessing, feature engineering, or encoding). Explain whether you would keep or revise that decision now that you know which model performs best. Justify your reasoning.\n",
    "  \n",
    "- Reconsider **at least one decision from Milestone 2** (e.g., model evaluation, cross-validation strategy, or feature selection). Again, explain whether you would keep or revise your original decision, and why.\n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset. This section should be a clean, readable summary of the most important steps from Milestones 1 and 2, adapted as needed to fit your final model choice and your reconsiderations as just described. \n",
    "\n",
    "- Use Markdown cells and inline comments to explain the structure of the code clearly but concisely. The goal is to make your reasoning and process easy to follow for instructors and reviewers.\n",
    "\n",
    "> Remember: You are not required to change your earlier choices, but you *are* required to reflect on them and justify your final decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd672725",
   "metadata": {},
   "source": [
    "## Project Setup: Imports, Configs, and Helper Functions\n",
    "\n",
    "This section loads all the libraries and settings we'll use throughout the project, grouped by purpose for clarity:\n",
    "\n",
    "### 1. Standard Libraries  \n",
    "General utilities for file handling, math, time tracking, and URL parsing (`os`, `time`, `math`, `io`, `zipfile`, `requests`, `urlparse`, `itertools`).\n",
    "\n",
    "### 2. Data Science Tools  \n",
    "`pandas`, `numpy` for data wrangling; `seaborn`, `matplotlib` for visualizations.\n",
    "\n",
    "### 3. Scikit-Learn (ML)  \n",
    "Covers preprocessing, model evaluation, feature selection, and ensemble models.\n",
    "\n",
    "### 4. Extras  \n",
    "`kagglehub` for downloading datasets, `tqdm` for progress bars.\n",
    "\n",
    "### 5. Global Config  \n",
    "Sets a random seed (`random_state = 42`) for reproducibility.\n",
    "\n",
    "### 6. Utility Functions  \n",
    "- `dollar_format`: formats y-axis ticks as dollar values.  \n",
    "- `format_hms`: converts seconds into `HH:MM:SS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be0244-e449-4322-a036-d188724901aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Useful Imports\n",
    "# =============================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Kaggle and Progress Tracking\n",
    "import kagglehub\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646a7c6",
   "metadata": {},
   "source": [
    "### Data Download & Load\n",
    "\n",
    "We use `load_zillow_data()` to check if the dataset exists locally, download it if not, and then load it as a DataFrame. It handles errors cleanly and avoids repeated downloads. This keeps our workflow smooth and avoids accidentally hitting the server more than needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ea78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zillow_data(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads a CSV file from the given URL if it doesn't exist locally,\n",
    "    then loads it into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL to the Zillow dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(urlparse(url).path)\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        try:\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Download complete.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download the file: {e}\")\n",
    "            raise\n",
    "\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/zillow_dataset.csv\"\n",
    "df = load_zillow_data(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f6f54e",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "\n",
    "We use the `explore_dataframe` function to understand the structure and quality of the dataset:\n",
    "- View the first few rows\n",
    "- Examine column data types and null counts\n",
    "- Identify unique values per column\n",
    "- Assess missing data percentages\n",
    "- Generate summary statistics for numeric features\n",
    "\n",
    "These steps help define cleaning and transformation strategies in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataframe(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Display essential exploration metrics for a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to explore.\n",
    "    \"\"\"\n",
    "    print(\"First five rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\nUnique values per column:\")\n",
    "    display(df.nunique().sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nMissing values (%):\")\n",
    "    missing_percent = df.isnull().mean().sort_values(ascending=False) * 100\n",
    "    display(missing_percent[missing_percent > 0])\n",
    "\n",
    "    print(\"\\nSummary statistics for numerical columns:\")\n",
    "    summary_stats = df.describe().T\n",
    "    summary_stats['std'] = summary_stats['std'].sort_values(ascending=False)\n",
    "    display(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186c5ed",
   "metadata": {},
   "source": [
    "### Visualizing Numeric Distributions\n",
    "\n",
    "The `plot_numeric_histograms()` function quickly gives us a look at the distribution of all numeric features. This helps spot skew, outliers, and potential candidates for transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_histograms(df: pd.DataFrame, bins: int = 50) -> None:\n",
    "    \"\"\"\n",
    "    Plots histograms for all numeric features in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to plot.\n",
    "        bins (int): Number of bins to use in each histogram.\n",
    "    \"\"\"\n",
    "    df.hist(figsize=(20, 15), bins=bins, layout=(-1, 5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_numeric_histograms(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4409e",
   "metadata": {},
   "source": [
    "### Identifying Low-Cardinality Categorical Features\n",
    "\n",
    "The `find_low_cardinality_categoricals()` function helps us pinpoint columns that are likely categorical (but maybe stored as object or even numeric types). These are good candidates for encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_low_cardinality_categoricals(df: pd.DataFrame, threshold: int = 10, show: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Identifies and optionally displays low-cardinality categorical features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to scan.\n",
    "        threshold (int): Max number of unique values to be considered \"low cardinality\".\n",
    "        show (bool): If True, print the feature names.\n",
    "\n",
    "    Returns:\n",
    "        list: Column names of low-cardinality categorical features.\n",
    "    \"\"\"\n",
    "    low_cardinality = df.nunique()[df.nunique() < threshold].index.tolist()\n",
    "    categoricals = [\n",
    "        col for col in low_cardinality \n",
    "        if df[col].dtype == \"object\" or df[col].dtype.name == \"category\"\n",
    "    ]\n",
    "    if show:\n",
    "        print(\"Low-cardinality categorical features:\")\n",
    "        print(categoricals)\n",
    "    return categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c5663",
   "metadata": {},
   "source": [
    "We could possibly remove the following variables simply due to sheer numbers of null presence:\n",
    "| Field Name                  | Non-Null Count | Total Count | Percentage (%) |\n",
    "|-----------------------------|----------------|-------------|----------------|\n",
    "| architecturalstyletypeid    | 207            | 77613       | 0.27           |\n",
    "| basementsqft                | 50             | 77613       | 0.06           |\n",
    "| buildingclasstypeid         | 15             | 77613       | 0.02           |\n",
    "| decktypeid                  | 614            | 77613       | 0.79           |\n",
    "| finishedfloor1squarefeet    | 6037           | 77613       | 7.78           |\n",
    "| finishedsquarefeet13        | 42             | 77613       | 0.05           |\n",
    "| finishedsquarefeet15        | 3027           | 77613       | 3.90           |\n",
    "| finishedsquarefeet50        | 6037           | 77613       | 7.78           |\n",
    "| finishedsquarefeet6         | 386            | 77613       | 0.50           |\n",
    "| poolsizesum                 | 869            | 77613       | 1.12           |\n",
    "| pooltypeid10                | 465            | 77613       | 0.60           |\n",
    "| pooltypeid2                 | 1074           | 77613       | 1.38           |\n",
    "| storytypeid                 | 50             | 77613       | 0.06           |\n",
    "| typeconstructiontypeid      | 223            | 77613       | 0.29           |\n",
    "| yardbuildingsqft17          | 2393           | 77613       | 3.08           |\n",
    "| yardbuildingsqft26          | 70             | 77613       | 0.09           |\n",
    "| fireplaceflag               | 172            | 77613       | 0.22           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2185e3",
   "metadata": {},
   "source": [
    "## Project Framing & Objectives\n",
    "\n",
    "### 1.1 What’s the business goal?  \n",
    "We’re building a regression model to predict property values using real estate data — things like square footage, location, and amenities. This supports smarter pricing and valuation tools.\n",
    "\n",
    "### 1.2 How will it be used?  \n",
    "Our model can help real estate professionals — especially marketing teams — make more data-driven decisions. Zillow (or similar platforms) could use it in automated valuation tools.\n",
    "\n",
    "### 1.3 How do we measure success?  \n",
    "We’ll track RMSE and R² on validation/test data. A strong model should generalize well, handle variation across cities/neighborhoods, and be easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c66abd",
   "metadata": {},
   "source": [
    "### Re-checking the Dataset Structure\n",
    "\n",
    "We revisit the basic structure, types, and missing values of the dataset. This helps confirm our earlier observations and catches any subtle changes after framing or minor cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dc9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run our EDA summary in case anything has changed\n",
    "explore_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613bf72",
   "metadata": {},
   "source": [
    "### 2.A.1: Which features are categorical?\n",
    "\n",
    "Features like `propertylandusetypeid`, `airconditioningtypeid`, and `regionidzip` are technically numeric, but function as categories. They represent types or group identifiers and should be encoded accordingly.\n",
    "\n",
    "### 2.A.2: Any features that seem useless?\n",
    "\n",
    "Yes — columns like `decktypeid`, `basementsqft`, and `fireplaceflag` have very few non-null entries and don't seem helpful for our valuation task. Unless we find they’re critical later, we’ll likely drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab96e3",
   "metadata": {},
   "source": [
    "### 2.A.3: Useless Features by Missingness\n",
    "\n",
    "Features like `basementsqft`, `yardbuildingsqft17`, and `finishedsquarefeet6` are missing in 99%+ of rows. That’s too sparse for reliable modeling, so we’ll consider dropping them.\n",
    "\n",
    "### 2.A.4: Useless Features by Uniqueness\n",
    "\n",
    "`parcelid` is a unique identifier for each row. Since it carries no generalizable pattern, we’ll exclude it from modeling.\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "We also noticed potential redundancy in some features, like square footage and bathroom count variations. These may be dropped or consolidated later during feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b7672",
   "metadata": {},
   "source": [
    "### Starting 2.B, Target + Predictor Distributions\n",
    "\n",
    "Boxplots help us spot outliers and skew in both the target (`taxvaluedollarcnt`) and predictors. These patterns influence how we scale or transform features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c518719",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m.hist(figsize=(\u001b[32m20\u001b[39m, \u001b[32m15\u001b[39m), bins=\u001b[32m50\u001b[39m, layout=(-\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      2\u001b[39m plt.tight_layout()\n\u001b[32m      3\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.hist(figsize=(20, 15), bins=50, layout=(-1, 5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_histograms(df: pd.DataFrame, bins: int = 50) -> None:\n",
    "    df.hist(figsize=(20, 15), bins=bins, layout=(-1, 5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d4ba9",
   "metadata": {},
   "source": [
    "### Target + Predictor Distributions\n",
    "\n",
    "Boxplots help us spot outliers and skew in both the target (`taxvaluedollarcnt`) and predictors. These patterns influence how we scale or transform features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_and_predictors(df: pd.DataFrame, target: str, predictors: list, show: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Plots boxplots for the target variable and selected key predictors.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset.\n",
    "        target (str): Name of the target variable.\n",
    "        predictors (list): List of column names to visualize alongside the target.\n",
    "        show (bool): Whether to display the plot.\n",
    "    \"\"\"\n",
    "    total_plots = len(predictors) + 1\n",
    "    rows = (total_plots + 1) // 2\n",
    "    fig, axes = plt.subplots(rows, 2, figsize=(14, rows * 4))\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    sns.boxplot(x=df[target], ax=axes[0])\n",
    "    axes[0].set_title(f\"Target: {target}\")\n",
    "\n",
    "    for i, col in enumerate(predictors):\n",
    "        sns.boxplot(x=df[col], ax=axes[i + 1])\n",
    "        axes[i + 1].set_title(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_target_and_predictors(\n",
    "    df,\n",
    "    target=\"taxvaluedollarcnt\",\n",
    "    predictors=[\"calculatedfinishedsquarefeet\", \"bathroomcnt\", \"bedroomcnt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949bb60",
   "metadata": {},
   "source": [
    "### 2.B Summary: Feature Distribution Observations\n",
    "\n",
    "- The target `taxvaluedollarcnt` is highly skewed (right-tailed), which suggests high-value outliers may affect regression.\n",
    "- `calculatedfinishedsquarefeet`, `bathroomcnt`, and `bedroomcnt` all show decent spread but possible skew or multiple modes.\n",
    "- We'll consider transformation or filtering based on these insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e22659",
   "metadata": {},
   "source": [
    "## Part 3: Clean the Data (Drop, Impute, Encode)\n",
    "\n",
    "From here on, we begin modifying the data for modeling. At each stage, we’ll use new variable names like `df_dropped`, `df_imputed`, and so on, to preserve version history and track changes step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef20bd1",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "Here we check for redundancy among feature groups using correlation matrices and visualizations. Highly correlated features (e.g., square footage, bathroom types) will be simplified by keeping the most complete column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c837edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reusable Function: Group Feature Correlation with Heatmap ===\n",
    "def check_feature_group_correlation(df: pd.DataFrame, features: list, group_name: str, show: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Displays a correlation matrix and heatmap for a group of features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset.\n",
    "        features (list): Features to compare.\n",
    "        group_name (str): Group label for plot titles.\n",
    "        show (bool): Show heatmap and plots.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Correlation matrix of selected features.\n",
    "    \"\"\"\n",
    "    subset = df[features].dropna(how='all')\n",
    "    corr = subset.corr()\n",
    "\n",
    "    if show:\n",
    "        print(f\"\\nCorrelation matrix for {group_name}:\")\n",
    "        display(corr)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.4f')\n",
    "        plt.title(f'Correlation Matrix: {group_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if len(features) == 2:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(data=subset, x=features[0], y=features[1], alpha=0.5)\n",
    "            plt.title(f'Scatter Plot: {features[0]} vs {features[1]}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Square Footage\n",
    "sqft_features = ['calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6']\n",
    "sqft_corr = check_feature_group_correlation(df, sqft_features, 'Square Footage')\n",
    "\n",
    "# 2. Bathroom Counts\n",
    "bath_features = ['bathroomcnt', 'calculatedbathnbr', 'fullbathcnt', 'threequarterbathnbr']\n",
    "bath_corr = check_feature_group_correlation(df, bath_features, 'Bathroom Features')\n",
    "\n",
    "# 3. Census Features\n",
    "census_features = ['rawcensustractandblock', 'censustractandblock']\n",
    "census_corr = check_feature_group_correlation(df, census_features, 'Census Features')\n",
    "\n",
    "# 4. Pool Features\n",
    "pool_features = ['poolcnt', 'pooltypeid7']\n",
    "pool_corr = check_feature_group_correlation(df, pool_features, 'Pool Features')\n",
    "\n",
    "# Extra Checks on Pool Data\n",
    "print(f\"\\nPercentage of properties with missing pool count: {df['poolcnt'].isnull().mean() * 100:.2f}%\")\n",
    "both_pool = (~df['poolcnt'].isnull()) & (~df['pooltypeid7'].isnull())\n",
    "print(f\"Properties with both pool features non-null: {both_pool.sum()}\")\n",
    "print(f\"Non-null poolcnt: {(~df['poolcnt'].isnull()).sum()}\")\n",
    "print(f\"Non-null pooltypeid7: {(~df['pooltypeid7'].isnull()).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21aa28",
   "metadata": {},
   "source": [
    "### Deep-Dive Pairwise Feature Comparisons\n",
    "\n",
    "To avoid dropping features that might be similar but not redundant, we use this detailed comparison. It checks:\n",
    "- Correlation coefficient\n",
    "- Whether they’re scaled versions of each other\n",
    "- Outlier examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac85d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reusable Function: Deep Pairwise Correlation Analysis ===\n",
    "def analyze_pairwise_correlation(df: pd.DataFrame, feature1: str, feature2: str, show: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Compares two features: correlation, scale similarity, and outlier differences.\n",
    "\n",
    "    Returns Pearson correlation and displays plots + diagnostics.\n",
    "    \"\"\"\n",
    "    # [PASTE full function code here... unchanged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c02f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Detailed Pairwise Correlation Analysis ===\n",
    "analyze_pairwise_correlation(df, 'calculatedfinishedsquarefeet', 'finishedsquarefeet12')\n",
    "analyze_pairwise_correlation(df, 'calculatedfinishedsquarefeet', 'finishedsquarefeet13')\n",
    "analyze_pairwise_correlation(df, 'calculatedfinishedsquarefeet', 'finishedsquarefeet15')\n",
    "analyze_pairwise_correlation(df, 'calculatedfinishedsquarefeet', 'finishedsquarefeet6')\n",
    "analyze_pairwise_correlation(df, 'calculatedbathnbr', 'bathroomcnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd143b",
   "metadata": {},
   "source": [
    "### Feature Dropping Based on Redundancy\n",
    "\n",
    "Based on correlation and completeness, we dropped features that are:\n",
    "- Functionally identical to others (`bathroomcnt`, `finishedsquarefeet12`)\n",
    "- Redundant geographic fields\n",
    "- Unique IDs or constants\n",
    "\n",
    "We keep the most complete and interpretable columns to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_irrelevant_features(df: pd.DataFrame, columns_to_drop: list, show: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops specified non-predictive or redundant features from the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        columns_to_drop (list): List of column names to remove.\n",
    "        show (bool): If True, prints list and new shape.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    df_reduced = df.drop(columns=columns_to_drop).copy()\n",
    "    if show:\n",
    "        print(\"Dropped irrelevant features:\", columns_to_drop)\n",
    "        print(\"New shape:\", df_reduced.shape)\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Drop irrelevant or redundant features after correlation analysis ===\n",
    "drop_irrelevant = [\n",
    "    \"parcelid\",                # Unique ID – not predictive\n",
    "    \"rawcensustractandblock\",  # High-cardinality\n",
    "    \"censustractandblock\",     # Redundant with above\n",
    "    \"regionidzip\",             # Geographic ID\n",
    "    \"assessmentyear\",          # Constant\n",
    "    \"finishedsquarefeet12\",    # Duplicate of calculatedfinishedsquarefeet\n",
    "    \"finishedsquarefeet13\",    # \"\"\n",
    "    \"finishedsquarefeet15\",    # \"\"\n",
    "    \"finishedsquarefeet6\",     # \"\"\n",
    "    \"bathroomcnt\"              # Duplicate of calculatedbathnbr\n",
    "]\n",
    "\n",
    "df_cleaned = drop_irrelevant_features(df, drop_irrelevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73dc3e",
   "metadata": {},
   "source": [
    "### 3.A: Justifying Feature Drops\n",
    "\n",
    "We dropped features that either:\n",
    "- Had no predictive value (`parcelid`)\n",
    "- Were constant (`assessmentyear`)\n",
    "- Were highly correlated and less complete (`bathroomcnt`, `finishedsquarefeet12`, etc.)\n",
    "\n",
    "Specifically:\n",
    "- `calculatedfinishedsquarefeet` was kept over similar features due to higher completeness (99.7% vs 95.2%)\n",
    "- `calculatedbathnbr` was kept for similar reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a64eda",
   "metadata": {},
   "source": [
    "### 3.B: Drop Features with Excessive Missingness\n",
    "\n",
    "Now we identify features with high missing values that also lack strong correlation to the target. These are unlikely to help the model and may introduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554ccb4",
   "metadata": {},
   "source": [
    "### Identifying Weak Features: High Nulls + Low Correlation\n",
    "\n",
    "This function highlights features that are both:\n",
    "- Highly incomplete (over 90% missing)\n",
    "- Weakly correlated to the target (abs(corr) < 0.1)\n",
    "\n",
    "These features likely won't help the model and may just add noise, so we’ll consider dropping them next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_high_null_low_corr_features(df: pd.DataFrame, target: str, null_thresh: float = 0.90, corr_thresh: float = 0.1, show: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies features with high null rates and low correlation with the target.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataset.\n",
    "        target (str): The target column to check correlation against.\n",
    "        null_thresh (float): Proportion of missing values allowed (e.g., 0.90).\n",
    "        corr_thresh (float): Absolute correlation threshold below which features are flagged.\n",
    "        show (bool): If True, displays summary DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of high-null, low-corr columns.\n",
    "    \"\"\"\n",
    "    high_null_cols = df.columns[df.isnull().mean() > null_thresh]\n",
    "    correlations = df.corr(numeric_only=True)[target]\n",
    "    low_corr_cols = correlations[correlations.abs() < corr_thresh].index\n",
    "    drop_candidates = list(set(high_null_cols) & set(low_corr_cols))\n",
    "\n",
    "    summary_data = []\n",
    "    for col in drop_candidates:\n",
    "        preview = df[col].dropna().unique()[:3]\n",
    "        summary_data.append({\n",
    "            \"Feature\": col,\n",
    "            \"Data Preview\": preview,\n",
    "            \"Correlation\": correlations.get(col),\n",
    "            \"Null Count\": df[col].isnull().sum(),\n",
    "            \"Non-Null Count\": df[col].notnull().sum()\n",
    "        })\n",
    "\n",
    "    drop_summary_df = pd.DataFrame(summary_data).sort_values(\n",
    "        by=\"Correlation\", key=lambda x: x.abs(), ascending=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    if show:\n",
    "        print(f\"\\nHigh-null, low-correlation candidates (null > {null_thresh*100:.0f}%, corr < {corr_thresh}):\")\n",
    "        display(drop_summary_df)\n",
    "\n",
    "    return drop_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.B – Use function to identify weak features\n",
    "drop_summary_df = identify_high_null_low_corr_features(df_cleaned, target=\"taxvaluedollarcnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f71991",
   "metadata": {},
   "source": [
    "### Dropping Weak Features\n",
    "\n",
    "Now that we’ve identified columns with both high missingness and little correlation to the target,\n",
    "we drop them from the dataset. This helps reduce noise and dimensionality before encoding or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_null_low_corr(\n",
    "    df: pd.DataFrame, \n",
    "    target: str, \n",
    "    null_thresh: float = 0.90, \n",
    "    corr_thresh: float = 0.10, \n",
    "    show: bool = True\n",
    ") -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"\n",
    "    Drops features that have a high proportion of missing values AND are weakly correlated with the target.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to process.\n",
    "        target (str): Name of the target column.\n",
    "        null_thresh (float): Null value threshold (default: 90%).\n",
    "        corr_thresh (float): Correlation threshold (default: 0.10).\n",
    "        show (bool): If True, print dropped features and reason.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (cleaned DataFrame, list of dropped column names)\n",
    "    \"\"\"\n",
    "    high_null = df.columns[df.isnull().mean() > null_thresh]\n",
    "    numeric_df = df.select_dtypes(include='number')\n",
    "    correlations = numeric_df.corr()[target].fillna(0)\n",
    "    low_corr = correlations[correlations.abs() < corr_thresh].index\n",
    "\n",
    "    to_drop = list(set(high_null) & set(low_corr))\n",
    "\n",
    "    if show:\n",
    "        print(f\"\\nDropping {len(to_drop)} features with >{null_thresh*100:.0f}% missing and low correlation:\")\n",
    "        for col in to_drop:\n",
    "            print(f\" - {col}: {df[col].isnull().mean()*100:.1f}% null, corr = {correlations[col]:.3f}\")\n",
    "\n",
    "    df_cleaned = df.drop(columns=to_drop).copy()\n",
    "    return df_cleaned, to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.B – Drop the features we identified\n",
    "df_null_cleaned, dropped_features = drop_high_null_low_corr(df_cleaned, target=\"taxvaluedollarcnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138a532",
   "metadata": {},
   "source": [
    "### Applying High-Null + Low-Correlation Drop\n",
    "\n",
    "We now run our drop function on `df_cleaned` to remove features that were:\n",
    "- More than 60–90% missing\n",
    "- Weakly correlated with the target (|corr| < 0.3)\n",
    "\n",
    "This produces a new version of the dataset: `df_null_cleaned`, which is smaller and cleaner moving into the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_correlations(df, target='taxvaluedollarcnt', top_n=10, method='pearson', figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Plot correlations between the target variable and its most correlated numeric features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input DataFrame (should be cleaned beforehand)\n",
    "    target : str, default='taxvaluedollarcnt'\n",
    "        The target variable to correlate against\n",
    "    top_n : int, default=10\n",
    "        Number of top features to include (excluding the target itself)\n",
    "    method : str, default='pearson'\n",
    "        Correlation method: 'pearson', 'kendall', or 'spearman'\n",
    "    figsize : tuple\n",
    "        Size of the output figure\n",
    "    \"\"\"\n",
    "    # Select numeric features\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Compute correlation with the target\n",
    "    target_corr = numeric_df.corrwith(df[target], method=method).sort_values(ascending=False)\n",
    "    \n",
    "    # Drop the target itself and select top N correlated features\n",
    "    top_features = target_corr.drop(index=target).head(top_n).index.tolist()\n",
    "    \n",
    "    # Build subset DataFrame with target + top features\n",
    "    subset_cols = [target] + top_features\n",
    "    corr_subset = numeric_df[subset_cols].corr(method=method)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        corr_subset,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap='coolwarm',\n",
    "        vmax=1.0,\n",
    "        vmin=-1.0,\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    plt.title(f\"Top {top_n} {method.capitalize()} Correlated Features with '{target}'\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "# ---- Usage with your own DataFrame ----\n",
    "top_features = plot_target_correlations(df_cleaned, top_n=10, method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.B – Apply feature drop for high-null + low-correlation fields\n",
    "df_null_cleaned, dropped_features = drop_high_null_low_corr(df_cleaned, target=\"taxvaluedollarcnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce548ba-c1ce-4a66-8885-9f7e40b1c404",
   "metadata": {},
   "source": [
    "## Part B: Final Data Science Project Report Assignment [30 pts]\n",
    "\n",
    "This final report is the culmination of your semester-long Data Science project, building upon the exploratory analyses and modeling milestones you've already completed. Your report should clearly communicate your findings, analysis approach, and conclusions to a technical audience. The following structure and guidelines, informed by best practices, will help you prepare a professional and comprehensive document.\n",
    "\n",
    "### Required Sections\n",
    "\n",
    "Your report must include the following sections:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d6069-cf27-4312-8e4b-8c27fa6cf9d0",
   "metadata": {},
   "source": [
    "#### 1. Executive Summary (Abstract) [2 pts]\n",
    "- Brief overview of the entire project (150–200 words)\n",
    "- Clearly state the objective, approach, and key findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51bc7a8-eac5-4dba-9052-d3fbf29adaf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5edd854f-857f-41a2-983a-845c99a87153",
   "metadata": {},
   "source": [
    "#### 2. Introduction [2 pts]\n",
    "- Clearly introduce the topic and context of your project\n",
    "- Describe the problem you are addressing (the problem statement)\n",
    "- Clearly state the objectives and goals of your analysis\n",
    "\n",
    "Note: You may imaginatively consider this project as taking place in a real estate company with a small data science group in-house, and write your introduction from this point of view (don't worry about verisimilitude to an actual company!).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf823c-453c-40ba-89f0-07826b9adf7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6394185-1992-469c-b475-05bd473327b1",
   "metadata": {},
   "source": [
    "#### 3. Data Description [2 pts]\n",
    "- Describe the source of your dataset (described in Milestone 1)\n",
    "- Clearly state the characteristics of your data (size, types of features, missing values, target, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47de29-d135-4312-a46b-97427c8b1ca4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db7d906a-bd9a-42f8-8eb4-7bba1b1b108a",
   "metadata": {},
   "source": [
    "#### 4. Methodology (What you did, and why)  [12 pts]\n",
    "\n",
    "**Focus this section entirely on the steps you took and your reasoning behind them. Emphasize the process and decision-making, not the results themselves**\n",
    "\n",
    "- Describe your analytical framework \n",
    "  - Use of validation curves to see the effect of various hyperparameter choices, and\n",
    "  - Choice of RMSE as primary error metric\n",
    "- Clearly outline your data cleaning and preprocessing steps\n",
    "  - Describe what issues you encountered in the raw data and how you addressed them.\n",
    "  - Mention any key decisions (e.g., removing samples with too many missing values).\n",
    "  - What worked and what didn't work?\n",
    "- Describe your feature engineering approach\n",
    "  - Explain any transformations, combinations, or derived features.\n",
    "  - Discuss why certain features were chosen or created, even if they were later discarded.\n",
    "  - What worked and what didn't work?\n",
    "- Detail your model selection process \n",
    "  - Outline the models you experimented with and why.\n",
    "  - Discuss how you evaluated generalization (e.g., cross-validation, shape and relationships of plots).\n",
    "  - Mention how you tuned hyperparameters or selected the final model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276de7c1-71ab-4bca-b1d4-e78979cbd0b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4305c55a-370f-4083-8cb6-395545ff1013",
   "metadata": {},
   "source": [
    "#### 5. Results and Evaluation (What you found, and how well it worked) [10 pts]\n",
    "\n",
    "**Focus purely on outcomes, with metrics, visuals, and insights. This is where you present evidence to support your conclusions.**\n",
    "\n",
    "- Provide a clear and detailed narrative of your analysis and reasoning using the analytical approach described in (4). \n",
    "- Discuss model performance metrics and results (RMSE, R2, etc.)\n",
    "- **Include relevant visualizations (graphs, charts, tables) with appropriate labels and captions**\n",
    "- Error analysis\n",
    "  - Highlight specific patterns of error, outliers, or questionable features.\n",
    "  - Note anything surprising or worth improving in future iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615213f-7689-47fe-ac5a-24a767faaae5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c97153d2-e099-4c15-99f8-ad0b6a539d4b",
   "metadata": {},
   "source": [
    "#### 6. Conclusion [2 pts]\n",
    "- Clearly state your main findings and how they address your original objectives\n",
    "- Highlight the business or practical implications of your findings \n",
    "- Discuss the limitations and constraints of your analysis clearly and transparently\n",
    "- Suggest potential improvements or future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287f955-7ae0-41ec-864d-44765c60ffe7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
