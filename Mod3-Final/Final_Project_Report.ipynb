{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706edba9-eb08-4f57-bcc5-a1bb7024f068",
   "metadata": {},
   "source": [
    "# Project Final Report\n",
    "\n",
    "### Due: Midnight on April 27 (2-hour grace period) — 50 points  \n",
    "\n",
    "### No late submissions will be accepted.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Your final submission consists of **three components**:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Final Report Notebook [40 pts]\n",
    "\n",
    "Complete all sections of this notebook to document your final decisions, results, and broader context.\n",
    "\n",
    "- **Part A**: Select the single best model from your Milestone 2 experiments. Now that you’ve finalized your model, revisit your decisions from Milestones 1 and 2. Are there any steps you would change—such as cleaning, feature engineering, or model evaluation—given what you now know?\n",
    "\n",
    "- **Part B**: Write a technical report following standard conventions, for example:\n",
    "  - [CMU guide to structure](https://www.stat.cmu.edu/~brian/701/notes/paper-structure.pdf)\n",
    "  - [Data science report example](https://www.projectpro.io/article/data-science-project-report/620)\n",
    "  - The Checklist given in this week's Blackboard Lesson (essentially the same as in HOML).\n",
    "    \n",
    "  Your audience here is technically literate but unfamiliar with your work—like your manager or other data scientists. Be clear, precise, and include both code (for illustration), charts/plots/illustrations, and explanation of what you discovered and your reasoning process. \n",
    "\n",
    "The idea here is that Part A would be a repository of the most important code, for further work to come, and Part B is\n",
    "the technical report which summarizes your project for the data science group at your company. Do NOT assume that readers of Part B are intimately familiar with Part A; provide code for illustration as needed, but not to run.\n",
    "\n",
    "Submit this notebook as a group via your team leader’s Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. PowerPoint Presentation [10 pts]\n",
    "\n",
    "Create a 10–15 minute presentation designed for a general audience (e.g., sales or marketing team).\n",
    "\n",
    "- Prepare 8–12 slides, following the general outline of the sections of Part B. \n",
    "- Focus on storytelling, visuals (plots and illustrations), and clear, simplified language. No code!\n",
    "- Use any presentation tool you like, but upload a PDF version.\n",
    "- List all team members on the first slide.\n",
    "\n",
    "Submit as a group via your team leader’s Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Individual Assessment\n",
    "\n",
    "Each team member must complete the Individual Assessment Form (same as in Milestone 1), sign it, and upload it via their own Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "-  Final Report Notebook — Team leader submission\n",
    "-  PDF Slides — Team leader submission\n",
    "-  Individual Assessment Form — Each member submits their own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee15ea-7cf4-4d57-b83a-18f186f0d204",
   "metadata": {},
   "source": [
    "## Part A: Final Model and Design Reassessment [10 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model and revisit earlier decisions to determine if any should be revised in light of your complete modeling workflow. You’ll also consolidate and present the key code used to run your model on the preprocessed dataset, with thoughtful documentation of your reasoning.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Reconsider **at least one decision from Milestone 1** (e.g., preprocessing, feature engineering, or encoding). Explain whether you would keep or revise that decision now that you know which model performs best. Justify your reasoning.\n",
    "  \n",
    "- Reconsider **at least one decision from Milestone 2** (e.g., model evaluation, cross-validation strategy, or feature selection). Again, explain whether you would keep or revise your original decision, and why.\n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset. This section should be a clean, readable summary of the most important steps from Milestones 1 and 2, adapted as needed to fit your final model choice and your reconsiderations as just described. \n",
    "\n",
    "- Use Markdown cells and inline comments to explain the structure of the code clearly but concisely. The goal is to make your reasoning and process easy to follow for instructors and reviewers.\n",
    "\n",
    "> Remember: You are not required to change your earlier choices, but you *are* required to reflect on them and justify your final decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd672725",
   "metadata": {},
   "source": [
    "## Project Setup: Imports, Configs, and Helper Functions\n",
    "\n",
    "This section loads all the libraries and settings we'll use throughout the project, grouped by purpose for clarity:\n",
    "\n",
    "### 1. Standard Libraries  \n",
    "General utilities for file handling, math, time tracking, and URL parsing (`os`, `time`, `math`, `io`, `zipfile`, `requests`, `urlparse`, `itertools`).\n",
    "\n",
    "### 2. Data Science Tools  \n",
    "`pandas`, `numpy` for data wrangling; `seaborn`, `matplotlib` for visualizations.\n",
    "\n",
    "### 3. Scikit-Learn (ML)  \n",
    "Covers preprocessing, model evaluation, feature selection, and ensemble models.\n",
    "\n",
    "### 4. Extras  \n",
    "`kagglehub` for downloading datasets, `tqdm` for progress bars.\n",
    "\n",
    "### 5. Global Config  \n",
    "Sets a random seed (`random_state = 42`) for reproducibility.\n",
    "\n",
    "### 6. Utility Functions  \n",
    "- `dollar_format`: formats y-axis ticks as dollar values.  \n",
    "- `format_hms`: converts seconds into `HH:MM:SS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be0244-e449-4322-a036-d188724901aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Useful Imports\n",
    "# =============================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Kaggle and Progress Tracking\n",
    "import kagglehub\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646a7c6",
   "metadata": {},
   "source": [
    "### Data Download & Load\n",
    "\n",
    "We use `load_zillow_data()` to check if the dataset exists locally, download it if not, and then load it as a DataFrame. It handles errors cleanly and avoids repeated downloads. This keeps our workflow smooth and avoids accidentally hitting the server more than needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ea78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zillow_data(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads a CSV file from the given URL if it doesn't exist locally,\n",
    "    then loads it into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL to the Zillow dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(urlparse(url).path)\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        try:\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Download complete.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download the file: {e}\")\n",
    "            raise\n",
    "\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")\n",
    "\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/zillow_dataset.csv\"\n",
    "df = load_zillow_data(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f6f54e",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "\n",
    "We use the `explore_dataframe` function to understand the structure and quality of the dataset:\n",
    "- View the first few rows\n",
    "- Examine column data types and null counts\n",
    "- Identify unique values per column\n",
    "- Assess missing data percentages\n",
    "- Generate summary statistics for numeric features\n",
    "\n",
    "These steps help define cleaning and transformation strategies in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataframe(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Display essential exploration metrics for a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to explore.\n",
    "    \"\"\"\n",
    "    print(\"First five rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\nUnique values per column:\")\n",
    "    display(df.nunique().sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nMissing values (%):\")\n",
    "    missing_percent = df.isnull().mean().sort_values(ascending=False) * 100\n",
    "    display(missing_percent[missing_percent > 0])\n",
    "\n",
    "    print(\"\\nSummary statistics for numerical columns:\")\n",
    "    summary_stats = df.describe().T\n",
    "    summary_stats['std'] = summary_stats['std'].sort_values(ascending=False)\n",
    "    display(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186c5ed",
   "metadata": {},
   "source": [
    "### Visualizing Numeric Distributions\n",
    "\n",
    "The `plot_numeric_histograms()` function quickly gives us a look at the distribution of all numeric features. This helps spot skew, outliers, and potential candidates for transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_histograms(df: pd.DataFrame, bins: int = 50) -> None:\n",
    "    \"\"\"\n",
    "    Plots histograms for all numeric features in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to plot.\n",
    "        bins (int): Number of bins to use in each histogram.\n",
    "    \"\"\"\n",
    "    df.hist(figsize=(20, 15), bins=bins, layout=(-1, 5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_numeric_histograms(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4409e",
   "metadata": {},
   "source": [
    "### Identifying Low-Cardinality Categorical Features\n",
    "\n",
    "The `find_low_cardinality_categoricals()` function helps us pinpoint columns that are likely categorical (but maybe stored as object or even numeric types). These are good candidates for encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_low_cardinality_categoricals(df: pd.DataFrame, threshold: int = 10, show: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Identifies and optionally displays low-cardinality categorical features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to scan.\n",
    "        threshold (int): Max number of unique values to be considered \"low cardinality\".\n",
    "        show (bool): If True, print the feature names.\n",
    "\n",
    "    Returns:\n",
    "        list: Column names of low-cardinality categorical features.\n",
    "    \"\"\"\n",
    "    low_cardinality = df.nunique()[df.nunique() < threshold].index.tolist()\n",
    "    categoricals = [\n",
    "        col for col in low_cardinality \n",
    "        if df[col].dtype == \"object\" or df[col].dtype.name == \"category\"\n",
    "    ]\n",
    "    if show:\n",
    "        print(\"Low-cardinality categorical features:\")\n",
    "        print(categoricals)\n",
    "    return categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c5663",
   "metadata": {},
   "source": [
    "We could possibly remove the following variables simply due to sheer numbers of null presence:\n",
    "| Field Name                  | Non-Null Count | Total Count | Percentage (%) |\n",
    "|-----------------------------|----------------|-------------|----------------|\n",
    "| architecturalstyletypeid    | 207            | 77613       | 0.27           |\n",
    "| basementsqft                | 50             | 77613       | 0.06           |\n",
    "| buildingclasstypeid         | 15             | 77613       | 0.02           |\n",
    "| decktypeid                  | 614            | 77613       | 0.79           |\n",
    "| finishedfloor1squarefeet    | 6037           | 77613       | 7.78           |\n",
    "| finishedsquarefeet13        | 42             | 77613       | 0.05           |\n",
    "| finishedsquarefeet15        | 3027           | 77613       | 3.90           |\n",
    "| finishedsquarefeet50        | 6037           | 77613       | 7.78           |\n",
    "| finishedsquarefeet6         | 386            | 77613       | 0.50           |\n",
    "| poolsizesum                 | 869            | 77613       | 1.12           |\n",
    "| pooltypeid10                | 465            | 77613       | 0.60           |\n",
    "| pooltypeid2                 | 1074           | 77613       | 1.38           |\n",
    "| storytypeid                 | 50             | 77613       | 0.06           |\n",
    "| typeconstructiontypeid      | 223            | 77613       | 0.29           |\n",
    "| yardbuildingsqft17          | 2393           | 77613       | 3.08           |\n",
    "| yardbuildingsqft26          | 70             | 77613       | 0.09           |\n",
    "| fireplaceflag               | 172            | 77613       | 0.22           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2185e3",
   "metadata": {},
   "source": [
    "## Project Framing & Objectives\n",
    "\n",
    "### 1.1 What’s the business goal?  \n",
    "We’re building a regression model to predict property values using real estate data — things like square footage, location, and amenities. This supports smarter pricing and valuation tools.\n",
    "\n",
    "### 1.2 How will it be used?  \n",
    "Our model can help real estate professionals — especially marketing teams — make more data-driven decisions. Zillow (or similar platforms) could use it in automated valuation tools.\n",
    "\n",
    "### 1.3 How do we measure success?  \n",
    "We’ll track RMSE and R² on validation/test data. A strong model should generalize well, handle variation across cities/neighborhoods, and be easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c66abd",
   "metadata": {},
   "source": [
    "### Re-checking the Dataset Structure\n",
    "\n",
    "We revisit the basic structure, types, and missing values of the dataset. This helps confirm our earlier observations and catches any subtle changes after framing or minor cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dc9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run our EDA summary in case anything has changed\n",
    "explore_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613bf72",
   "metadata": {},
   "source": [
    "### 2.A.1: Which features are categorical?\n",
    "\n",
    "Features like `propertylandusetypeid`, `airconditioningtypeid`, and `regionidzip` are technically numeric, but function as categories. They represent types or group identifiers and should be encoded accordingly.\n",
    "\n",
    "### 2.A.2: Any features that seem useless?\n",
    "\n",
    "Yes — columns like `decktypeid`, `basementsqft`, and `fireplaceflag` have very few non-null entries and don't seem helpful for our valuation task. Unless we find they’re critical later, we’ll likely drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab96e3",
   "metadata": {},
   "source": [
    "### 2.A.3: Useless Features by Missingness\n",
    "\n",
    "Features like `basementsqft`, `yardbuildingsqft17`, and `finishedsquarefeet6` are missing in 99%+ of rows. That’s too sparse for reliable modeling, so we’ll consider dropping them.\n",
    "\n",
    "### 2.A.4: Useless Features by Uniqueness\n",
    "\n",
    "`parcelid` is a unique identifier for each row. Since it carries no generalizable pattern, we’ll exclude it from modeling.\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "We also noticed potential redundancy in some features, like square footage and bathroom count variations. These may be dropped or consolidated later during feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b7672",
   "metadata": {},
   "source": [
    "### Starting 2.B, Target + Predictor Distributions\n",
    "\n",
    "Boxplots help us spot outliers and skew in both the target (`taxvaluedollarcnt`) and predictors. These patterns influence how we scale or transform features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c518719",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m.hist(figsize=(\u001b[32m20\u001b[39m, \u001b[32m15\u001b[39m), bins=\u001b[32m50\u001b[39m, layout=(-\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      2\u001b[39m plt.tight_layout()\n\u001b[32m      3\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.hist(figsize=(20, 15), bins=50, layout=(-1, 5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_histograms(df: pd.DataFrame, bins: int = 50) -> None:\n",
    "    df.hist(figsize=(20, 15), bins=bins, layout=(-1, 5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d4ba9",
   "metadata": {},
   "source": [
    "### Target + Predictor Distributions\n",
    "\n",
    "Boxplots help us spot outliers and skew in both the target (`taxvaluedollarcnt`) and predictors. These patterns influence how we scale or transform features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_and_predictors(df: pd.DataFrame, target: str, predictors: list, show: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Plots boxplots for the target variable and selected key predictors.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset.\n",
    "        target (str): Name of the target variable.\n",
    "        predictors (list): List of column names to visualize alongside the target.\n",
    "        show (bool): Whether to display the plot.\n",
    "    \"\"\"\n",
    "    total_plots = len(predictors) + 1\n",
    "    rows = (total_plots + 1) // 2\n",
    "    fig, axes = plt.subplots(rows, 2, figsize=(14, rows * 4))\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    sns.boxplot(x=df[target], ax=axes[0])\n",
    "    axes[0].set_title(f\"Target: {target}\")\n",
    "\n",
    "    for i, col in enumerate(predictors):\n",
    "        sns.boxplot(x=df[col], ax=axes[i + 1])\n",
    "        axes[i + 1].set_title(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_target_and_predictors(\n",
    "    df,\n",
    "    target=\"taxvaluedollarcnt\",\n",
    "    predictors=[\"calculatedfinishedsquarefeet\", \"bathroomcnt\", \"bedroomcnt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949bb60",
   "metadata": {},
   "source": [
    "### 2.B Summary: Feature Distribution Observations\n",
    "\n",
    "- The target `taxvaluedollarcnt` is highly skewed (right-tailed), which suggests high-value outliers may affect regression.\n",
    "- `calculatedfinishedsquarefeet`, `bathroomcnt`, and `bedroomcnt` all show decent spread but possible skew or multiple modes.\n",
    "- We'll consider transformation or filtering based on these insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e22659",
   "metadata": {},
   "source": [
    "## Part 3: Clean the Data (Drop, Impute, Encode)\n",
    "\n",
    "From here on, we begin modifying the data for modeling. At each stage, we’ll use new variable names like `df_dropped`, `df_imputed`, and so on, to preserve version history and track changes step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef20bd1",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "Here we check for redundancy among feature groups using correlation matrices and visualizations. Highly correlated features (e.g., square footage, bathroom types) will be simplified by keeping the most complete column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c837edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reusable Function: Group Feature Correlation with Heatmap ===\n",
    "def check_feature_group_correlation(df: pd.DataFrame, features: list, group_name: str, show: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Displays a correlation matrix and heatmap for a group of features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset.\n",
    "        features (list): Features to compare.\n",
    "        group_name (str): Group label for plot titles.\n",
    "        show (bool): Show heatmap and plots.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Correlation matrix of selected features.\n",
    "    \"\"\"\n",
    "    subset = df[features].dropna(how='all')\n",
    "    corr = subset.corr()\n",
    "\n",
    "    if show:\n",
    "        print(f\"\\nCorrelation matrix for {group_name}:\")\n",
    "        display(corr)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.4f')\n",
    "        plt.title(f'Correlation Matrix: {group_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if len(features) == 2:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(data=subset, x=features[0], y=features[1], alpha=0.5)\n",
    "            plt.title(f'Scatter Plot: {features[0]} vs {features[1]}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Square Footage\n",
    "sqft_features = ['calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6']\n",
    "sqft_corr = check_feature_group_correlation(df, sqft_features, 'Square Footage')\n",
    "\n",
    "# 2. Bathroom Counts\n",
    "bath_features = ['bathroomcnt', 'calculatedbathnbr', 'fullbathcnt', 'threequarterbathnbr']\n",
    "bath_corr = check_feature_group_correlation(df, bath_features, 'Bathroom Features')\n",
    "\n",
    "# 3. Census Features\n",
    "census_features = ['rawcensustractandblock', 'censustractandblock']\n",
    "census_corr = check_feature_group_correlation(df, census_features, 'Census Features')\n",
    "\n",
    "# 4. Pool Features\n",
    "pool_features = ['poolcnt', 'pooltypeid7']\n",
    "pool_corr = check_feature_group_correlation(df, pool_features, 'Pool Features')\n",
    "\n",
    "# Extra Checks on Pool Data\n",
    "print(f\"\\nPercentage of properties with missing pool count: {df['poolcnt'].isnull().mean() * 100:.2f}%\")\n",
    "both_pool = (~df['poolcnt'].isnull()) & (~df['pooltypeid7'].isnull())\n",
    "print(f\"Properties with both pool features non-null: {both_pool.sum()}\")\n",
    "print(f\"Non-null poolcnt: {(~df['poolcnt'].isnull()).sum()}\")\n",
    "print(f\"Non-null pooltypeid7: {(~df['pooltypeid7'].isnull()).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21aa28",
   "metadata": {},
   "source": [
    "### Deep-Dive Pairwise Feature Comparisons\n",
    "\n",
    "To avoid dropping features that might be similar but not redundant, we use this detailed comparison. It checks:\n",
    "- Correlation coefficient\n",
    "- Whether they’re scaled versions of each other\n",
    "- Outlier examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac85d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Reusable Function: Deep Pairwise Correlation Analysis ===\n",
    "def analyze_pairwise_correlation(df: pd.DataFrame, feature1: str, feature2: str, show: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Compares two features: correlation, scale similarity, and outlier differences.\n",
    "\n",
    "    Returns Pearson correlation and displays plots + diagnostics.\n",
    "    \"\"\"\n",
    "    # [PASTE full function code here... unchanged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c02f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Detailed Pairwise Correlation Analysis ===\n",
    "analyze_pairwise_correlation(df, 'calculatedfinishedsquarefeet', 'finishedsquarefeet12')\n",
    "analyze_pairwise_correlation(df, 'calculatedfinishedsquarefeet', 'finishedsquarefeet13')\n",
    "analyze_pairwise_correlation(df, 'calculatedfinishedsquarefeet', 'finishedsquarefeet15')\n",
    "analyze_pairwise_correlation(df, 'calculatedfinishedsquarefeet', 'finishedsquarefeet6')\n",
    "analyze_pairwise_correlation(df, 'calculatedbathnbr', 'bathroomcnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd143b",
   "metadata": {},
   "source": [
    "### Feature Dropping Based on Redundancy\n",
    "\n",
    "Based on correlation and completeness, we dropped features that are:\n",
    "- Functionally identical to others (`bathroomcnt`, `finishedsquarefeet12`)\n",
    "- Redundant geographic fields\n",
    "- Unique IDs or constants\n",
    "\n",
    "We keep the most complete and interpretable columns to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_irrelevant_features(df: pd.DataFrame, columns_to_drop: list, show: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops specified non-predictive or redundant features from the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        columns_to_drop (list): List of column names to remove.\n",
    "        show (bool): If True, prints list and new shape.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    df_reduced = df.drop(columns=columns_to_drop).copy()\n",
    "    if show:\n",
    "        print(\"Dropped irrelevant features:\", columns_to_drop)\n",
    "        print(\"New shape:\", df_reduced.shape)\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Drop irrelevant or redundant features after correlation analysis ===\n",
    "drop_irrelevant = [\n",
    "    \"parcelid\",                # Unique ID – not predictive\n",
    "    \"rawcensustractandblock\",  # High-cardinality\n",
    "    \"censustractandblock\",     # Redundant with above\n",
    "    \"regionidzip\",             # Geographic ID\n",
    "    \"assessmentyear\",          # Constant\n",
    "    \"finishedsquarefeet12\",    # Duplicate of calculatedfinishedsquarefeet\n",
    "    \"finishedsquarefeet13\",    # \"\"\n",
    "    \"finishedsquarefeet15\",    # \"\"\n",
    "    \"finishedsquarefeet6\",     # \"\"\n",
    "    \"bathroomcnt\"              # Duplicate of calculatedbathnbr\n",
    "]\n",
    "\n",
    "df_cleaned = drop_irrelevant_features(df, drop_irrelevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73dc3e",
   "metadata": {},
   "source": [
    "### 3.A: Justifying Feature Drops\n",
    "\n",
    "We dropped features that either:\n",
    "- Had no predictive value (`parcelid`)\n",
    "- Were constant (`assessmentyear`)\n",
    "- Were highly correlated and less complete (`bathroomcnt`, `finishedsquarefeet12`, etc.)\n",
    "\n",
    "Specifically:\n",
    "- `calculatedfinishedsquarefeet` was kept over similar features due to higher completeness (99.7% vs 95.2%)\n",
    "- `calculatedbathnbr` was kept for similar reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a64eda",
   "metadata": {},
   "source": [
    "### 3.B: Drop Features with Excessive Missingness\n",
    "\n",
    "Now we identify features with high missing values that also lack strong correlation to the target. These are unlikely to help the model and may introduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554ccb4",
   "metadata": {},
   "source": [
    "### Identifying Weak Features: High Nulls + Low Correlation\n",
    "\n",
    "This function highlights features that are both:\n",
    "- Highly incomplete (over 90% missing)\n",
    "- Weakly correlated to the target (abs(corr) < 0.1)\n",
    "\n",
    "These features likely won't help the model and may just add noise, so we’ll consider dropping them next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_high_null_low_corr_features(df: pd.DataFrame, target: str, null_thresh: float = 0.90, corr_thresh: float = 0.1, show: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies features with high null rates and low correlation with the target.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataset.\n",
    "        target (str): The target column to check correlation against.\n",
    "        null_thresh (float): Proportion of missing values allowed (e.g., 0.90).\n",
    "        corr_thresh (float): Absolute correlation threshold below which features are flagged.\n",
    "        show (bool): If True, displays summary DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of high-null, low-corr columns.\n",
    "    \"\"\"\n",
    "    high_null_cols = df.columns[df.isnull().mean() > null_thresh]\n",
    "    correlations = df.corr(numeric_only=True)[target]\n",
    "    low_corr_cols = correlations[correlations.abs() < corr_thresh].index\n",
    "    drop_candidates = list(set(high_null_cols) & set(low_corr_cols))\n",
    "\n",
    "    summary_data = []\n",
    "    for col in drop_candidates:\n",
    "        preview = df[col].dropna().unique()[:3]\n",
    "        summary_data.append({\n",
    "            \"Feature\": col,\n",
    "            \"Data Preview\": preview,\n",
    "            \"Correlation\": correlations.get(col),\n",
    "            \"Null Count\": df[col].isnull().sum(),\n",
    "            \"Non-Null Count\": df[col].notnull().sum()\n",
    "        })\n",
    "\n",
    "    drop_summary_df = pd.DataFrame(summary_data).sort_values(\n",
    "        by=\"Correlation\", key=lambda x: x.abs(), ascending=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    if show:\n",
    "        print(f\"\\nHigh-null, low-correlation candidates (null > {null_thresh*100:.0f}%, corr < {corr_thresh}):\")\n",
    "        display(drop_summary_df)\n",
    "\n",
    "    return drop_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.B – Use function to identify weak features\n",
    "drop_summary_df = identify_high_null_low_corr_features(df_cleaned, target=\"taxvaluedollarcnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f71991",
   "metadata": {},
   "source": [
    "### Dropping Weak Features\n",
    "\n",
    "Now that we’ve identified columns with both high missingness and little correlation to the target,\n",
    "we drop them from the dataset. This helps reduce noise and dimensionality before encoding or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_null_low_corr(\n",
    "    df: pd.DataFrame, \n",
    "    target: str, \n",
    "    null_thresh: float = 0.90, \n",
    "    corr_thresh: float = 0.10, \n",
    "    show: bool = True\n",
    ") -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"\n",
    "    Drops features that have a high proportion of missing values AND are weakly correlated with the target.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to process.\n",
    "        target (str): Name of the target column.\n",
    "        null_thresh (float): Null value threshold (default: 90%).\n",
    "        corr_thresh (float): Correlation threshold (default: 0.10).\n",
    "        show (bool): If True, print dropped features and reason.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (cleaned DataFrame, list of dropped column names)\n",
    "    \"\"\"\n",
    "    high_null = df.columns[df.isnull().mean() > null_thresh]\n",
    "    numeric_df = df.select_dtypes(include='number')\n",
    "    correlations = numeric_df.corr()[target].fillna(0)\n",
    "    low_corr = correlations[correlations.abs() < corr_thresh].index\n",
    "\n",
    "    to_drop = list(set(high_null) & set(low_corr))\n",
    "\n",
    "    if show:\n",
    "        print(f\"\\nDropping {len(to_drop)} features with >{null_thresh*100:.0f}% missing and low correlation:\")\n",
    "        for col in to_drop:\n",
    "            print(f\" - {col}: {df[col].isnull().mean()*100:.1f}% null, corr = {correlations[col]:.3f}\")\n",
    "\n",
    "    df_cleaned = df.drop(columns=to_drop).copy()\n",
    "    return df_cleaned, to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.B – Drop the features we identified\n",
    "df_null_cleaned, dropped_features = drop_high_null_low_corr(df_cleaned, target=\"taxvaluedollarcnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138a532",
   "metadata": {},
   "source": [
    "### Applying High-Null + Low-Correlation Drop\n",
    "\n",
    "We now run our drop function on `df_cleaned` to remove features that were:\n",
    "- More than 60–90% missing\n",
    "- Weakly correlated with the target (|corr| < 0.3)\n",
    "\n",
    "This produces a new version of the dataset: `df_null_cleaned`, which is smaller and cleaner moving into the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_correlations(df, target='taxvaluedollarcnt', top_n=10, method='pearson', figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Plot correlations between the target variable and its most correlated numeric features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input DataFrame (should be cleaned beforehand)\n",
    "    target : str, default='taxvaluedollarcnt'\n",
    "        The target variable to correlate against\n",
    "    top_n : int, default=10\n",
    "        Number of top features to include (excluding the target itself)\n",
    "    method : str, default='pearson'\n",
    "        Correlation method: 'pearson', 'kendall', or 'spearman'\n",
    "    figsize : tuple\n",
    "        Size of the output figure\n",
    "    \"\"\"\n",
    "    # Select numeric features\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Compute correlation with the target\n",
    "    target_corr = numeric_df.corrwith(df[target], method=method).sort_values(ascending=False)\n",
    "    \n",
    "    # Drop the target itself and select top N correlated features\n",
    "    top_features = target_corr.drop(index=target).head(top_n).index.tolist()\n",
    "    \n",
    "    # Build subset DataFrame with target + top features\n",
    "    subset_cols = [target] + top_features\n",
    "    corr_subset = numeric_df[subset_cols].corr(method=method)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        corr_subset,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap='coolwarm',\n",
    "        vmax=1.0,\n",
    "        vmin=-1.0,\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    plt.title(f\"Top {top_n} {method.capitalize()} Correlated Features with '{target}'\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "# ---- Usage with your own DataFrame ----\n",
    "top_features = plot_target_correlations(df_cleaned, top_n=10, method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.B – Apply feature drop for high-null + low-correlation fields\n",
    "df_null_cleaned, dropped_features = drop_high_null_low_corr(df_cleaned, target=\"taxvaluedollarcnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0266cd89",
   "metadata": {},
   "source": [
    "### Null Value Summary\n",
    "\n",
    "This function gives a quick overview of how much missing data remains in the current dataset.  \n",
    "Here, we filter to only show columns with more than 1% missing values, helping us decide whether to impute or drop in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace61172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_nulls(df: pd.DataFrame, sort_by: str = 'percent', threshold: float = 0.0, show: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarizes null values in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to analyze.\n",
    "        sort_by (str): 'percent' or 'count' — column to sort by.\n",
    "        threshold (float): Minimum % of nulls to include in output.\n",
    "        show (bool): If True, displays the summary.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Table showing columns with null counts and percentages.\n",
    "    \"\"\"\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_percents = null_counts / len(df) * 100\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Column': null_counts.index,\n",
    "        'Null Count': null_counts.values,\n",
    "        'Null Percent': null_percents.round(2),\n",
    "        'Data Type': df.dtypes.astype(str).values\n",
    "    })\n",
    "\n",
    "    filtered_df = summary_df[summary_df['Null Percent'] > threshold * 100]\n",
    "\n",
    "    sort_column = 'Null Count' if sort_by == 'count' else 'Null Percent'\n",
    "    result = filtered_df.sort_values(by=sort_column, ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if show:\n",
    "        display(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e288d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining nulls after drop step\n",
    "summarize_nulls(df_null_cleaned, threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd5091",
   "metadata": {},
   "source": [
    "### Top Correlated Features\n",
    "\n",
    "This heatmap shows which features are most linearly related to our target (`taxvaluedollarcnt`).\n",
    "We'll use this as a guide when choosing features for modeling — strong correlations are good indicators for prediction potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_correlations(\n",
    "    df: pd.DataFrame,\n",
    "    target: str = 'taxvaluedollarcnt',\n",
    "    top_n: int = 10,\n",
    "    method: str = 'pearson',\n",
    "    figsize: tuple = (12, 10),\n",
    "    show: bool = True\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the top N features most correlated with the target variable.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "        target (str): Target column name.\n",
    "        top_n (int): Number of top correlated features to show.\n",
    "        method (str): Correlation method ('pearson', 'spearman', etc.).\n",
    "        figsize (tuple): Size of the heatmap figure.\n",
    "        show (bool): If True, displays the heatmap.\n",
    "\n",
    "    Returns:\n",
    "        list: Top N feature names most correlated with the target.\n",
    "    \"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    target_corr = numeric_df.corrwith(df[target], method=method).sort_values(ascending=False)\n",
    "\n",
    "    # Drop the target itself from results\n",
    "    top_features = target_corr.drop(index=target).head(top_n).index.tolist()\n",
    "    corr_subset = numeric_df[[target] + top_features].corr(method=method)\n",
    "\n",
    "    if show:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(\n",
    "            corr_subset, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
    "            vmax=1.0, vmin=-1.0, center=0, square=True, linewidths=0.5\n",
    "        )\n",
    "        plt.title(f\"Top {top_n} {method.capitalize()} Correlated Features with '{target}'\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 10 most correlated features\n",
    "top_features = plot_target_correlations(df_null_cleaned, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1c50a",
   "metadata": {},
   "source": [
    "### Why We Added Feature Variance Testing\n",
    "\n",
    "Testing for feature variance helps identify columns that are nearly constant — meaning they offer little to no value to the model. These features may still have numeric values, but their lack of diversity makes them poor predictors.\n",
    "\n",
    "### Removing Low-Variance Features\n",
    "\n",
    "We use `VarianceThreshold` to filter out numeric features with almost no variance. These are unlikely to help in modeling and can contribute noise or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def remove_low_variance_features(df: pd.DataFrame, threshold: float = 0.0, show: bool = True) -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"\n",
    "    Removes features with variance below the specified threshold.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to evaluate (numeric columns only).\n",
    "        threshold (float): Variance threshold.\n",
    "        show (bool): If True, prints removed columns.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (Cleaned DataFrame, List of dropped column names)\n",
    "    \"\"\"\n",
    "    numeric_df = df.select_dtypes(include='number')\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(numeric_df)\n",
    "\n",
    "    retained = numeric_df.columns[selector.get_support()]\n",
    "    dropped = list(set(numeric_df.columns) - set(retained))\n",
    "\n",
    "    df_reduced = df.drop(columns=dropped)\n",
    "\n",
    "    if show:\n",
    "        print(f\"Dropped {len(dropped)} low-variance features:\")\n",
    "        for col in dropped:\n",
    "            print(f\" - {col}\")\n",
    "\n",
    "    return df_reduced, dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b724723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.B continued — remove low-variance columns from the cleaned dataset\n",
    "df_var_filtered, low_var_cols = remove_low_variance_features(df_null_cleaned, threshold=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2640280",
   "metadata": {},
   "source": [
    "### Full Correlation Heatmap\n",
    "\n",
    "This heatmap provides a bird’s-eye view of correlations across all numeric features. It’s useful for spotting redundant predictors or multicollinearity risks before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdfb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_full_correlation_matrix(df: pd.DataFrame, method: str = 'pearson', figsize: tuple = (12, 10), show: bool = True):\n",
    "    \"\"\"\n",
    "    Plots a full correlation heatmap for all numeric features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset to plot.\n",
    "        method (str): Correlation method ('pearson', 'spearman', etc.).\n",
    "        figsize (tuple): Figure size.\n",
    "        show (bool): If True, shows the heatmap.\n",
    "    \"\"\"\n",
    "    corr = df.select_dtypes(include='number').corr(method=method)\n",
    "\n",
    "    if show:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(corr, annot=False, cmap='coolwarm', square=True, linewidths=0.5)\n",
    "        plt.title(f'Full Correlation Matrix ({method.capitalize()})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot full correlation matrix after feature selection\n",
    "plot_full_correlation_matrix(df_var_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57ce67",
   "metadata": {},
   "source": [
    "### 3.B Discussion: Feature Dropping Rationale\n",
    "\n",
    "We dropped features with more than 60% missing data and correlation below 0.3 with the target.  \n",
    "This included features like `architecturalstyletypeid`, which add little predictive value due to sparsity.  \n",
    "The goal was to reduce noise and simplify the dataset without sacrificing useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14242eb6",
   "metadata": {},
   "source": [
    "### Part 3.C: Drop Problematic Samples\n",
    "\n",
    "We now evaluate row-level issues like:\n",
    "- Missing target values\n",
    "- Excessive missing features in a row\n",
    "- Outliers in key columns (especially the target)\n",
    "\n",
    "These samples may introduce unreliable patterns or skew our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4fb46",
   "metadata": {},
   "source": [
    "### Converting Features to Categorical Types\n",
    "\n",
    "Some numeric columns (like ZIP code or property type) are actually categorical.  \n",
    "We explicitly convert them to save memory and help future encoding.  \n",
    "Examples include `regionidzip`, `propertylandusetypeid`, and `unitcnt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c14502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_report_categoricals(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Converts known numeric columns to categorical based on domain logic.\n",
    "    Also reports memory savings and dtype changes.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    converted_cols = []\n",
    "\n",
    "    memory_before = df_copy.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    categorical_candidates = [\n",
    "        'fips', 'buildingqualitytypeid', 'heatingorsystemtypeid',\n",
    "        'propertylandusetypeid', 'regionidcity', 'regionidcounty',\n",
    "        'regionidzip', 'unitcnt', 'yearbuilt'\n",
    "    ]\n",
    "\n",
    "    for col in categorical_candidates:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            n_unique = df[col].nunique()\n",
    "            null_count = df[col].isnull().sum()\n",
    "\n",
    "            if n_unique <= 255:\n",
    "                df_copy[col] = df[col].astype('category')\n",
    "                conversion_type = 'category'\n",
    "            elif col == 'yearbuilt':\n",
    "                df_copy[col] = df[col].astype('Int64')\n",
    "                conversion_type = 'Int64'\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            converted_cols.append({\n",
    "                'column': col,\n",
    "                'unique_values': n_unique,\n",
    "                'null_values': null_count,\n",
    "                'conversion_type': conversion_type\n",
    "            })\n",
    "\n",
    "    memory_after = df_copy.memory_usage(deep=True).sum() / 1024**2\n",
    "    conversion_info = {\n",
    "        'total_conversions': len(converted_cols),\n",
    "        'converted_columns': converted_cols,\n",
    "        'memory_before_MB': round(memory_before, 2),\n",
    "        'memory_after_MB': round(memory_after, 2)\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nConverted {len(converted_cols)} columns:\")\n",
    "        for info in converted_cols:\n",
    "            print(f\"  - {info['column']}: {info['conversion_type']} ({info['unique_values']} unique, {info['null_values']} nulls)\")\n",
    "        print(f\"\\nMemory reduced from {memory_before:.2f} MB to {memory_after:.2f} MB\")\n",
    "\n",
    "    return df_copy, conversion_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert domain-informed features to categoricals\n",
    "df_cleaned, conversion_info = convert_and_report_categoricals(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bca7ed",
   "metadata": {},
   "source": [
    "### Dropping Target Outliers\n",
    "\n",
    "We use the IQR method to remove extreme values from the `taxvaluedollarcnt` column.  \n",
    "This helps reduce skew and make the model more robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_iqr_outliers(df, target=\"taxvaluedollarcnt\", iqr_multiplier=1.5, verbose=True):\n",
    "    \"\"\"\n",
    "    Drops rows where the target variable is an outlier based on IQR.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "        target (str): Target column to filter on.\n",
    "        iqr_multiplier (float): Controls strictness of bounds.\n",
    "        verbose (bool): If True, prints summary stats.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame without target outliers.\n",
    "    \"\"\"\n",
    "    Q1 = df[target].quantile(0.25)\n",
    "    Q3 = df[target].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - iqr_multiplier * IQR\n",
    "    upper = Q3 + iqr_multiplier * IQR\n",
    "\n",
    "    df_filtered = df[(df[target] >= lower) & (df[target] <= upper)]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "        print(f\"Filtered shape: {df_filtered.shape}\")\n",
    "        print(f\"Outliers removed: {df.shape[0] - df_filtered.shape[0]}\")\n",
    "        print(f\"IQR bounds: {lower:.2f} to {upper:.2f}\")\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from the target column using IQR\n",
    "df_samples = drop_iqr_outliers(df_cleaned, target=\"taxvaluedollarcnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436de23",
   "metadata": {},
   "source": [
    "### 3.C Discussion: Sample Dropping\n",
    "\n",
    "We removed rows that:\n",
    "- Had more than 60% missing values\n",
    "- Contained missing target values\n",
    "- Had extreme target outliers based on IQR\n",
    "\n",
    "This prevents unreliable or overly influential samples from skewing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c864110",
   "metadata": {},
   "source": [
    "### Conservative Imputation\n",
    "\n",
    "Only features with a logical, low-risk imputation path were filled — such as:\n",
    "- `lotsizesquarefeet` → filled with median\n",
    "- `propertycountylandusecode` → filled with mode\n",
    "\n",
    "We avoid imputing binary flags or high-missing columns to prevent introducing artificial bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conservative_impute(df: pd.DataFrame, strategy_map: dict = None, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing values conservatively using predefined strategies for each feature.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        strategy_map (dict): Optional manual overrides of imputation strategy per column.\n",
    "        verbose (bool): If True, prints imputed columns and strategies.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with selected features imputed.\n",
    "    \"\"\"\n",
    "    df_filled = df.copy()\n",
    "    imputed = {}\n",
    "\n",
    "    # Default strategies if not provided\n",
    "    default_strategy = {\n",
    "        'median': ['lotsizesquarefeet', 'landtaxvaluedollarcnt'],\n",
    "        'mode': ['propertycountylandusecode', 'regionidcounty']\n",
    "    }\n",
    "\n",
    "    if strategy_map:\n",
    "        for k in strategy_map:\n",
    "            default_strategy[k] = strategy_map[k]\n",
    "\n",
    "    for method, cols in default_strategy.items():\n",
    "        for col in cols:\n",
    "            if col in df_filled.columns and df_filled[col].isnull().any():\n",
    "                if method == 'median':\n",
    "                    fill_value = df_filled[col].median()\n",
    "                elif method == 'mode':\n",
    "                    fill_value = df_filled[col].mode()[0]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                df_filled[col] = df_filled[col].fillna(fill_value)\n",
    "                imputed[col] = method\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Imputed columns:\")\n",
    "        for col, method in imputed.items():\n",
    "            print(f\"  - {col} ({method})\")\n",
    "\n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.D – Apply conservative imputation logic\n",
    "df_imputed = conservative_impute(df_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17b39b",
   "metadata": {},
   "source": [
    "#### **3.D Discussion:**  \n",
    "We were highly selective about which missing values to fill. Imputation was only applied where it was safe, such as using median for continuous variables and mode for stable categorical labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c5248",
   "metadata": {},
   "source": [
    "There were very few imputations that felt safe to make without introducing bias.  \n",
    "The sensitive binary-like flags (`fireplaceflag`, `hashottuborspa`, `taxdelinquencyflag`) were left untouched because they are sparse and potentially meaningful in their null state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff7efe",
   "metadata": {},
   "source": [
    "### Features with High Missing Values and Reasons for Not Imputing\n",
    "\n",
    "| **Feature**                  | **Definition**                                                                 | **% Missing** | **Reason for Not Imputing**                                                                                                                                                                                                                 |\n",
    "|------------------------------|--------------------------------------------------------------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `buildingclasstypeid`        | Classification of the building (e.g., wood frame, masonry).                     | High         | Imputing could misrepresent the structural integrity and classification of the property, leading to inaccurate assessments.                                                                                                                  |\n",
    "| `finishedsquarefeet6`        | Base unfinished and finished area (e.g., basement and garage).                  | High         | Imputation could lead to incorrect assumptions about the property's usable space, affecting valuation and analysis.                                                                                                                         |\n",
    "| `storytypeid`                | Type of stories in the building (e.g., basement, attic).                        | High         | Imputing this feature could misrepresent the architectural style and functionality of the property, leading to inaccurate assessments.                                                                                                       |\n",
    "| `basementsqft`               | Basement square footage.                                                        | High         | Not all properties have basements; imputing could falsely suggest the presence or absence of a basement, leading to incorrect valuations.                                                                                                   |\n",
    "| `finishedsquarefeet15`       | Total finished living area including additions.                                 | High         | Imputation could lead to incorrect assumptions about the property's size and value, as additions vary widely between properties.                                                                                                            |\n",
    "| `fireplaceflag`              | Flag indicating the presence of a fireplace.                                    | High         | Imputing could lead to incorrect assumptions about the property's features, as the presence of a fireplace can significantly impact a property's appeal and value.                                                                           |\n",
    "| `pooltypeid10`               | Indicates the presence of a spa or hot tub.                                     | High         | Imputing could lead to incorrect assumptions about luxury amenities, which significantly impact property valuation and buyer interest.                                                                                                      |\n",
    "| `decktypeid`                 | Type of deck present on the property.                                           | High         | Imputation could misrepresent the property's outdoor features, leading to inaccurate assessments of its amenities and value.                                                                                                                |\n",
    "| `pooltypeid2`                | Indicates the presence of a pool with a spa.                                    | High         | Similar to `pooltypeid10`, imputing could lead to incorrect assumptions about luxury amenities, affecting property valuation.                                                                                                               |\n",
    "| `hashottuborspa`             | Indicates if the property has a hot tub or spa.                                 | High         | Imputing could lead to false assumptions about the presence of luxury amenities, which can significantly influence property valuation and buyer interest.                                                                                   |\n",
    "| `poolsizesum`                | Total square footage of all pools on the property.                              | High         | Imputation could lead to inaccurate representations of the property's features, as pool size varies widely and significantly affects property value.                                                                                        |\n",
    "| `taxdelinquencyflag`         | Flag indicating if the property has any tax delinquencies.                      | High         | Imputing could misrepresent the financial status of the property, leading to inaccurate risk assessments.                                                                                                                                   |\n",
    "| `yardbuildingsqft17`         | Square footage of sheds (non-living areas).                                     | High         | Not all properties have sheds; imputing could falsely suggest the presence or absence of such structures, affecting property assessments.                                                                                                   |\n",
    "| `threequarterbathnbr`        | Number of 3/4 bathrooms in the property.                                        | High         | Imputation could lead to inaccurate representations of the property's amenities, as the number of bathrooms is a critical factor in property valuation.                                                                                     |\n",
    "| `finishedsquarefeet50`       | Finished square feet of the first floor.                                        | High         | Imputing could lead to incorrect assumptions about the property's layout and size, affecting valuation and analysis.                                                                                                                        |\n",
    "| `finishedfloor1squarefeet`   | Finished square feet of the first floor.                                        | High         | Similar to `finishedsquarefeet50`, imputation could misrepresent the property's layout and size, leading to inaccurate assessments.                                                                                                         |\n",
    "| `fireplacecnt`               | Number of fireplaces in the property.                                           | High         | Imputing could lead to false assumptions about the property's features, as the number of fireplaces can significantly impact a property's appeal and value.                                                                                 |\n",
    "| `numberofstories`            | Number of stories in the building.                                              | High         | Imputation could misrepresent the property's structure, leading to inaccurate assessments of its size and value.                                                                                                                            |\n",
    "| `garagetotalsqft`            | Total square footage of the garage.                                             | High         | Not all properties have garages; imputing could falsely suggest the presence or size of a garage, affecting property valuation.                                                                                                             |\n",
    "| `garagecarcnt`               | Number of cars the garage can accommodate.                                      | High         | Similar to `garagetotalsqft`, imputation could misrepresent the property's features, leading to inaccurate assessments of its amenities and value.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d653c",
   "metadata": {},
   "source": [
    "### Evaluation of Candidate Categorical Features\n",
    "\n",
    "| **Feature**                | **Real Meaning**                         | **Continuous or Categorical?** | **Should Be Encoded?** | **Reason** |\n",
    "|---------------------------|------------------------------------------|-------------------------------|------------------------|------------|\n",
    "| `fips`                    | County code                              | Categorical                   | Yes                    | Region ID; numeric format misleading — not ordinal or continuous |\n",
    "| `buildingqualitytypeid`   | Building quality score (1–10 scale)       | Ordinal categorical           | Yes                    | Small set of levels indicating quality rank — encode as ordinal or category |\n",
    "| `heatingorsystemtypeid`   | Heating system type                      | Categorical                   | Yes                    | Discrete set of system codes; not continuous |\n",
    "| `propertylandusetypeid`   | Land use classification                  | Categorical                   | Yes                    | Maps to property types (e.g., single-family, duplex) |\n",
    "| `regionidcity`            | City code                                | Categorical                   | Yes (carefully)        | High-cardinality location identifier — label encoding preferred |\n",
    "| `regionidcounty`          | County code                              | Categorical                   | Yes                    | Fewer unique values — safe to encode |\n",
    "| `regionidzip`             | ZIP code                                 | Categorical                   | Yes (carefully)        | High-cardinality; useful as a location marker but must avoid treating as numeric |\n",
    "| `unitcnt`                 | Number of units (e.g., 1 for SFR, 2+ for multi-unit) | Discrete numeric              | Optional               | Could remain numeric, or encode if it's categorical for model context |\n",
    "| `yearbuilt`              | Year the property was built               | Temporal (numeric)            | **No**                 | Represents a timeline — has potential linear relationship with valuation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad596fa",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features\n",
    "\n",
    "We used `OrdinalEncoder` to convert all remaining categorical features to numeric codes.  \n",
    "This helps models work with these features, especially tree-based models that don’t require one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def encode_categoricals(df: pd.DataFrame, dtype_include=['object', 'category'], verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encodes categorical features using ordinal encoding.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with potential categoricals.\n",
    "        dtype_include (list): Data types to treat as categorical.\n",
    "        verbose (bool): Whether to print encoded columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with encoded features.\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    categorical_cols = df.select_dtypes(include=dtype_include).columns.tolist()\n",
    "\n",
    "    if verbose and categorical_cols:\n",
    "        print(f\"Encoding {len(categorical_cols)} categorical columns:\")\n",
    "        print(categorical_cols)\n",
    "\n",
    "    if categorical_cols:\n",
    "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        df_encoded[categorical_cols] = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.E – Encode categoricals\n",
    "df_encoded = encode_categoricals(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b534c1",
   "metadata": {},
   "source": [
    "### Setup for Part 4 - Model Dataset Preparation\n",
    "\n",
    "We create a clean copy of the encoded dataset and split it into:\n",
    "- `X`: all features\n",
    "- `y`: the target\n",
    "\n",
    "This allows us to begin evaluating model-driven feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba64f56",
   "metadata": {},
   "source": [
    "### Correlation Heatmap (Top Predictors)\n",
    "\n",
    "Here we visualize the 15 features most strongly correlated with the target.  \n",
    "This helps validate that our encoding + cleaning preserved useful signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for modeling and feature selection\n",
    "df_analysis = df_encoded.copy()\n",
    "X = df_analysis.drop(columns=[\"taxvaluedollarcnt\"])\n",
    "y = df_analysis[\"taxvaluedollarcnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse our top-N target correlation plot\n",
    "plot_target_correlations(df_analysis, target=\"taxvaluedollarcnt\", top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae0895",
   "metadata": {},
   "source": [
    "### Feature Validation\n",
    "\n",
    "Before we proceed with feature selection, we confirm the presence of the target and isolate only numeric columns for model-based evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36543015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure target is present\n",
    "assert \"taxvaluedollarcnt\" in df_analysis.columns, \"Target column not found.\"\n",
    "\n",
    "# Filter numeric features for model-based selection\n",
    "numeric_df = df_analysis.select_dtypes(include='number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c843a",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "\n",
    "We apply forward selection using linear regression to find the best minimal feature set.  \n",
    "This helps identify which combinations of predictors explain the most variance in the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "def forward_feature_selection(X, y, model, scoring='neg_root_mean_squared_error', cv=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Applies forward sequential feature selection using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Input features.\n",
    "        y (pd.Series): Target.\n",
    "        model: A sklearn estimator.\n",
    "        scoring (str): Scoring function.\n",
    "        cv (int): Cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "        list: Best feature subset, scores, and top-performing feature set.\n",
    "    \"\"\"\n",
    "    sfs = SequentialFeatureSelector(model, direction='forward', scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    sfs.fit(X, y)\n",
    "    selected = list(X.columns[sfs.get_support()])\n",
    "    score = cross_val_score(model, X[selected], y, scoring=scoring, cv=cv).mean()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Selected Features: {selected}\")\n",
    "        print(f\"Mean CV Score: {score:.4f}\")\n",
    "\n",
    "    return selected, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc16177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Run forward selection\n",
    "model = LinearRegression()\n",
    "forward_features, forward_score = forward_feature_selection(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5f704",
   "metadata": {},
   "source": [
    "### Backward Feature Selection\n",
    "\n",
    "We also test backward elimination to see which features are least important to model performance.  \n",
    "Comparing both methods helps ensure our selected features are both efficient and informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_feature_selection(X, y, model, scoring='neg_root_mean_squared_error', cv=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Applies backward sequential feature selection using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature set.\n",
    "        y (pd.Series): Target variable.\n",
    "        model: sklearn estimator.\n",
    "        scoring (str): Scoring function for evaluation.\n",
    "        cv (int): Cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "        list: Best-performing feature subset, score.\n",
    "    \"\"\"\n",
    "    sfs = SequentialFeatureSelector(model, direction='backward', scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    sfs.fit(X, y)\n",
    "    selected = list(X.columns[sfs.get_support()])\n",
    "    score = cross_val_score(model, X[selected], y, scoring=scoring, cv=cv).mean()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Selected Features (Backward): {selected}\")\n",
    "        print(f\"Mean CV Score: {score:.4f}\")\n",
    "\n",
    "    return selected, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7366fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backward selection\n",
    "model = LinearRegression()\n",
    "backward_features, backward_score = backward_feature_selection(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548e386",
   "metadata": {},
   "source": [
    "#### **4.A Discussion:** \n",
    "We preserved null values in our dataset for modeling integrity, but temporarily imputed them using median values for the purpose of computing F-statistics. This allowed us to evaluate the predictive strength of each feature without distorting the original data. The correlation matrix and F-statistics consistently identified square footage, bathroom count, and bedroom count as the strongest predictors of property value. We can also see this through the forward and backward selection processes as well. An interesting thing to note with forward selection, while it still valued the previously mentioned features, it considered other values better such as latitude, longitude, yearbuilt, and roomcnt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61960a",
   "metadata": {},
   "source": [
    "### 2D Feature Relationships\n",
    "\n",
    "- **SqFt vs Value**: Strong positive relationship\n",
    "- **Bathrooms vs Value**: Diminishing returns after 3 bathrooms\n",
    "- **Geo Distribution**: High-value homes cluster geographically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Visualizations: Value vs Key Features\n",
    "\n",
    "# Plot 1: Home Value vs. SqFt\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.hexbin(df_encoded['calculatedfinishedsquarefeet'], df_encoded['taxvaluedollarcnt'], gridsize=50, cmap='viridis', mincnt=1, linewidths=0.1)\n",
    "plt.colorbar(label='Density')\n",
    "plt.title('Home Value vs. SqFt')\n",
    "plt.xlabel('Calculated Finished SqFt')\n",
    "plt.ylabel('Tax Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Bathrooms vs Home Value (Density)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.kdeplot(data=df_encoded, x='calculatedbathnbr', y='taxvaluedollarcnt', fill=True, cmap=\"mako\", bw_adjust=0.7, thresh=0.05)\n",
    "plt.title('Bathrooms vs. Home Value (Density)')\n",
    "plt.xlabel('Bathroom Count')\n",
    "plt.ylabel('Tax Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Geographic Distribution by Home Value\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(data=df_encoded, x='longitude', y='latitude', hue='taxvaluedollarcnt', palette='viridis', alpha=0.6, edgecolor=None, legend=False)\n",
    "plt.title('Geographic Distribution by Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50f700",
   "metadata": {},
   "source": [
    "### Logarithmic Transformations\n",
    "\n",
    "Log transformations reduce skewness in key features like home value and lot size, making relationships more linear for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb227681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation\n",
    "skewed_features = ['taxvaluedollarcnt', 'calculatedfinishedsquarefeet', 'lotsizesquarefeet']\n",
    "df_log = apply_log_transform(df_analysis, skewed_features)\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(len(skewed_features), 2, figsize=(15, 4*len(skewed_features)))\n",
    "\n",
    "for i, feature in enumerate(skewed_features):\n",
    "    sns.histplot(df_analysis[feature], kde=True, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f'Original: {feature}')\n",
    "\n",
    "    sns.histplot(df_log[f'log_{feature}'], kde=True, ax=axes[i, 1])\n",
    "    axes[i, 1].set_title(f'Log-Transformed: {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b598d6a0",
   "metadata": {},
   "source": [
    "### Ratio Features\n",
    "\n",
    "We engineer new ratio features, like price per square foot and bathroom-to-bedroom ratios, to reveal new predictive patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a083bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratio_features(df):\n",
    "    \"\"\"\n",
    "    Creates engineered ratio features like price per sqft, bath/bed ratio, structure-to-lot ratio.\n",
    "    \"\"\"\n",
    "    df_ratio = df.copy()\n",
    "    df_ratio['price_per_sqft'] = df['taxvaluedollarcnt'] / df['calculatedfinishedsquarefeet']\n",
    "    df_ratio['bath_bed_ratio'] = df['calculatedbathnbr'] / (df['bedroomcnt'] + 0.01)\n",
    "    df_ratio['structure_lot_ratio'] = df['calculatedfinishedsquarefeet'] / (df['lotsizesquarefeet'] + 0.01)\n",
    "    return df_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ratio transformations\n",
    "df_ratio = create_ratio_features(df_analysis)\n",
    "\n",
    "# Visualize ratio feature distributions\n",
    "ratio_features = ['price_per_sqft', 'bath_bed_ratio', 'structure_lot_ratio']\n",
    "\n",
    "fig, axes = plt.subplots(len(ratio_features), 1, figsize=(10, 4*len(ratio_features)))\n",
    "\n",
    "for i, feature in enumerate(ratio_features):\n",
    "    q1 = df_ratio[feature].quantile(0.01)\n",
    "    q3 = df_ratio[feature].quantile(0.99)\n",
    "    filtered = df_ratio[(df_ratio[feature] >= q1) & (df_ratio[feature] <= q3)]\n",
    "    sns.histplot(filtered[feature], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    if feature == 'bath_bed_ratio':\n",
    "        axes[i].set_xlim(0.25, 2.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlations\n",
    "ratio_corr = df_ratio[ratio_features].corrwith(df_ratio['taxvaluedollarcnt'])\n",
    "print(\"\\nCorrelation of Ratio Features with Tax Value:\")\n",
    "print(ratio_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60721a8",
   "metadata": {},
   "source": [
    "### Polynomial and Interaction Features\n",
    "\n",
    "We create polynomial and interaction terms from key numeric features to capture non-linear and synergistic effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_polynomial_features(df, degree=2):\n",
    "    \"\"\"\n",
    "    Generates polynomial and interaction features from selected numerics.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset.\n",
    "        degree (int): Degree of polynomial features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with expanded polynomial features.\n",
    "        list: Feature names created.\n",
    "    \"\"\"\n",
    "    key_features = ['calculatedfinishedsquarefeet', 'calculatedbathnbr', 'bedroomcnt', 'lotsizesquarefeet']\n",
    "    df_subset = df[key_features]\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    poly_features = poly.fit_transform(df_subset)\n",
    "    feature_names = poly.get_feature_names_out(key_features)\n",
    "    \n",
    "    df_poly = pd.DataFrame(poly_features, columns=feature_names, index=df.index)\n",
    "    df_combined = pd.concat([df.drop(key_features, axis=1), df_poly], axis=1)\n",
    "    return df_combined, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ab606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply polynomial transformations\n",
    "df_poly, poly_feature_names = create_polynomial_features(df_analysis)\n",
    "\n",
    "# Evaluate subset of polynomial features\n",
    "interaction_features = [f for f in poly_feature_names if ' ' in f]\n",
    "squared_features = [f for f in poly_feature_names if '^2' in f]\n",
    "\n",
    "selected_poly_features = interaction_features[:5] + squared_features[:3]\n",
    "\n",
    "# Correlation\n",
    "poly_corr = df_poly[selected_poly_features].corrwith(df_poly['taxvaluedollarcnt'])\n",
    "print(\"\\nCorrelation of Polynomial Features with Tax Value:\")\n",
    "print(poly_corr.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67463861",
   "metadata": {},
   "source": [
    "### Evaluating Transformations\n",
    "\n",
    "We compute the average absolute correlation between the target and the features created by different transformations (log, ratio, polynomial).  \n",
    "This helps determine which transformations offer the best potential for predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(df_original, df_log, df_ratio, df_poly, target='taxvaluedollarcnt'):\n",
    "    \"\"\"\n",
    "    Compares basic, log, ratio, and polynomial transformations based on correlation with the target.\n",
    "\n",
    "    Args:\n",
    "        df_original (pd.DataFrame): Base dataset.\n",
    "        df_log (pd.DataFrame): Dataset with log-transformed features.\n",
    "        df_ratio (pd.DataFrame): Dataset with ratio features.\n",
    "        df_poly (pd.DataFrame): Dataset with polynomial features.\n",
    "        target (str): Target column.\n",
    "\n",
    "    Returns:\n",
    "        dict: Comparison of transformation types based on mean correlation scores.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Evaluate original numeric features\n",
    "    original_corr = df_original.select_dtypes(include='number').drop(columns=[target]).corrwith(df_original[target]).abs()\n",
    "    results['Original Features'] = original_corr.mean()\n",
    "\n",
    "    # Evaluate log-transformed features\n",
    "    log_cols = [col for col in df_log.columns if col.startswith('log_')]\n",
    "    log_corr = df_log[log_cols].corrwith(df_log[target]).abs()\n",
    "    results['Log-Transformed Features'] = log_corr.mean()\n",
    "\n",
    "    # Evaluate ratio features\n",
    "    ratio_cols = ['price_per_sqft', 'bath_bed_ratio', 'structure_lot_ratio']\n",
    "    ratio_corr = df_ratio[ratio_cols].corrwith(df_ratio[target]).abs()\n",
    "    results['Ratio Features'] = ratio_corr.mean()\n",
    "\n",
    "    # Evaluate polynomial features\n",
    "    poly_cols = [col for col in df_poly.columns if col not in df_original.columns and col != target]\n",
    "    poly_corr = df_poly[poly_cols].corrwith(df_poly[target]).abs()\n",
    "    results['Polynomial Features'] = poly_corr.mean()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 – Compare transformation effectiveness\n",
    "transformation_scores = evaluate_transformations(df_analysis, df_log, df_ratio, df_poly)\n",
    "\n",
    "print(\"\\nAverage Absolute Correlation by Feature Type:\")\n",
    "for transform, score in transformation_scores.items():\n",
    "    print(f\"{transform}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34b083",
   "metadata": {},
   "source": [
    "#### **5 Discussion:**\n",
    "We selected these three feature engineering transformations to capture different aspects of the property data that raw features alone might not fully express.  \n",
    "- The ratio features (price per square foot, bath/bedroom ratio) provided intuitive metrics for property value.\n",
    "- The log transformations helped linearize highly skewed variables.\n",
    "- Polynomial features captured non-linear interactions between building size, lot size, and bathroom count.\n",
    "\n",
    "From the visualizations, we observed that **ratio features performed exceptionally well**, showing the strongest correlations with tax value.  \n",
    "The price per square foot feature especially captured a strong, interpretable signal and will be prioritized in modeling experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa04503",
   "metadata": {},
   "source": [
    "## Foreword\n",
    "Looking forward, the team decided that the features below may introduce bias or noise to any models we build, and may be worth looking into as we progress with this milestone project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04135357",
   "metadata": {},
   "source": [
    "| **Feature**                       | **Potential Bias Type**                                               | **Summary**                                                                                     |\n",
    "|-----------------------------------|----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| **Region-related Features**       | **Location-based Bias**                                               | Features like `regionidcity` and `regionidcounty` reflect specific regions that might skew property pricing. Consider grouping regions or using one-hot encoding. |\n",
    "| **Tax Delinquency Flag**          | **Selection Bias**                                                    | Properties with tax delinquencies may not represent the general market, leading to skewed predictions. This feature may need to be excluded or handled carefully. |\n",
    "| **Year Built**                    | **Age-related Bias**                                                  | The year a property was built may introduce age-based bias, especially if newer properties are consistently priced higher. Consider categorizing properties into age groups. |\n",
    "| **Room Count and Bedroom Count**  | **Multicollinearity**                                                 | These features are highly correlated with square footage, which could lead to multicollinearity. Consider combining features or reducing them to avoid redundancy. |\n",
    "| **Basement and Garage Square Feet** | **Multicollinearity and Extreme Values**                             | These features might introduce bias if certain property types (e.g., those with garages or basements) are overrepresented. Consider creating binary features indicating their presence. |\n",
    "| **Finished Square Feet**          | **Multicollinearity**                                                 | Strongly correlated with other size-related features. Reduce redundancy by using one feature or transforming others. |\n",
    "| **Building Quality Type**         | **Encoding Bias**                                                     | Quality-related codes could introduce bias based on subjective perceptions of quality. Use categorical encoding techniques like one-hot encoding. |\n",
    "| **Zoning and Property Land Use**  | **Categorical Bias**                                                  | Land-use and zoning features may introduce bias due to regional regulations or property type distinctions. Use proper encoding (e.g., one-hot encoding) for these categorical features. |\n",
    "| **Fireplace Flag and Hot Tub/Spa Flag** | **Selection Bias**                                               | Binary flags for special features (e.g., fireplace, hot tub) might be skewed in specific market segments. Consider excluding or analyzing their impact carefully. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce548ba-c1ce-4a66-8885-9f7e40b1c404",
   "metadata": {},
   "source": [
    "## Part B: Final Data Science Project Report Assignment [30 pts]\n",
    "\n",
    "This final report is the culmination of your semester-long Data Science project, building upon the exploratory analyses and modeling milestones you've already completed. Your report should clearly communicate your findings, analysis approach, and conclusions to a technical audience. The following structure and guidelines, informed by best practices, will help you prepare a professional and comprehensive document.\n",
    "\n",
    "### Required Sections\n",
    "\n",
    "Your report must include the following sections:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d6069-cf27-4312-8e4b-8c27fa6cf9d0",
   "metadata": {},
   "source": [
    "#### 1. Executive Summary (Abstract) [2 pts]\n",
    "- Brief overview of the entire project (150–200 words)\n",
    "- Clearly state the objective, approach, and key findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51bc7a8-eac5-4dba-9052-d3fbf29adaf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5edd854f-857f-41a2-983a-845c99a87153",
   "metadata": {},
   "source": [
    "#### 2. Introduction [2 pts]\n",
    "- Clearly introduce the topic and context of your project\n",
    "- Describe the problem you are addressing (the problem statement)\n",
    "- Clearly state the objectives and goals of your analysis\n",
    "\n",
    "Note: You may imaginatively consider this project as taking place in a real estate company with a small data science group in-house, and write your introduction from this point of view (don't worry about verisimilitude to an actual company!).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf823c-453c-40ba-89f0-07826b9adf7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6394185-1992-469c-b475-05bd473327b1",
   "metadata": {},
   "source": [
    "#### 3. Data Description [2 pts]\n",
    "- Describe the source of your dataset (described in Milestone 1)\n",
    "- Clearly state the characteristics of your data (size, types of features, missing values, target, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47de29-d135-4312-a46b-97427c8b1ca4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db7d906a-bd9a-42f8-8eb4-7bba1b1b108a",
   "metadata": {},
   "source": [
    "#### 4. Methodology (What you did, and why)  [12 pts]\n",
    "\n",
    "**Focus this section entirely on the steps you took and your reasoning behind them. Emphasize the process and decision-making, not the results themselves**\n",
    "\n",
    "- Describe your analytical framework \n",
    "  - Use of validation curves to see the effect of various hyperparameter choices, and\n",
    "  - Choice of RMSE as primary error metric\n",
    "- Clearly outline your data cleaning and preprocessing steps\n",
    "  - Describe what issues you encountered in the raw data and how you addressed them.\n",
    "  - Mention any key decisions (e.g., removing samples with too many missing values).\n",
    "  - What worked and what didn't work?\n",
    "- Describe your feature engineering approach\n",
    "  - Explain any transformations, combinations, or derived features.\n",
    "  - Discuss why certain features were chosen or created, even if they were later discarded.\n",
    "  - What worked and what didn't work?\n",
    "- Detail your model selection process \n",
    "  - Outline the models you experimented with and why.\n",
    "  - Discuss how you evaluated generalization (e.g., cross-validation, shape and relationships of plots).\n",
    "  - Mention how you tuned hyperparameters or selected the final model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276de7c1-71ab-4bca-b1d4-e78979cbd0b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4305c55a-370f-4083-8cb6-395545ff1013",
   "metadata": {},
   "source": [
    "#### 5. Results and Evaluation (What you found, and how well it worked) [10 pts]\n",
    "\n",
    "**Focus purely on outcomes, with metrics, visuals, and insights. This is where you present evidence to support your conclusions.**\n",
    "\n",
    "- Provide a clear and detailed narrative of your analysis and reasoning using the analytical approach described in (4). \n",
    "- Discuss model performance metrics and results (RMSE, R2, etc.)\n",
    "- **Include relevant visualizations (graphs, charts, tables) with appropriate labels and captions**\n",
    "- Error analysis\n",
    "  - Highlight specific patterns of error, outliers, or questionable features.\n",
    "  - Note anything surprising or worth improving in future iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615213f-7689-47fe-ac5a-24a767faaae5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c97153d2-e099-4c15-99f8-ad0b6a539d4b",
   "metadata": {},
   "source": [
    "#### 6. Conclusion [2 pts]\n",
    "- Clearly state your main findings and how they address your original objectives\n",
    "- Highlight the business or practical implications of your findings \n",
    "- Discuss the limitations and constraints of your analysis clearly and transparently\n",
    "- Suggest potential improvements or future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287f955-7ae0-41ec-864d-44765c60ffe7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
